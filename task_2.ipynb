{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24c1d4a4",
   "metadata": {},
   "source": [
    "CADEC extractor — notebook-only library (no .py references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8e2bf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os, re, json\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Iterable, List, Optional, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a231c2d1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# --- optional: load .env if present (safe in notebooks)\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32fae588",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# --- env sanitization (prevents latin-1 header errors in HTTP headers)\n",
    "def _clean_ascii_header_value(s: str) -> str:\n",
    "    s = (s or \"\")\n",
    "    s = s.replace(\"Bearer \", \"\")\n",
    "    s = s.strip().strip('\"').strip(\"'\")\n",
    "    s = re.sub(r\"[^\\x20-\\x7E]\", \"\", s)  # printable ASCII only\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c094226",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_MODEL = os.getenv(\"DEFAULT_MODEL\", \"llama-3.1-8b-instant\")\n",
    "GROQ_BASEURL = os.getenv(\"GROQ_BASEURL\", \"https://api.groq.com/openai/v1\")\n",
    "GROQ_API_KEY = _clean_ascii_header_value(os.getenv(\"GROQ_API_KEY\") or \"\")\n",
    "if not GROQ_API_KEY:\n",
    "    raise RuntimeError(\"GROQ_API_KEY is missing/invalid. Set it in the notebook env or .env (GROQ_API_KEY=sk_...).\")\n",
    "os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY  # keep sanitized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63d09bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CHARS = 1500\n",
    "TIMEOUT = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdc512a3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# --- labels & priority\n",
    "BASE_LABELS = (\"ADR\", \"Drug\", \"Disease\", \"Symptom\")\n",
    "PRIORITY_BASE = {\"ADR\": 4, \"Drug\": 3, \"Disease\": 2, \"Symptom\": 1}\n",
    "PRIORITY_WITH_FINDING = {\"ADR\": 5, \"Drug\": 4, \"Disease\": 3, \"Symptom\": 2, \"Finding\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b3b0c23",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# --- data\n",
    "@dataclass\n",
    "class Range:\n",
    "    start: int\n",
    "    end: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ea3f10b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RawSpan:\n",
    "    label: str\n",
    "    ranges: List[Range]\n",
    "    text: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "560f009e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Span:\n",
    "    label: str\n",
    "    ranges: List[Range]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4c8c7a3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# --- prompt builder\n",
    "def build_system_prompt(keep_finding: bool, allow_procedures: bool, distance_policy: str) -> str:\n",
    "    labels_list = list(BASE_LABELS)\n",
    "    if keep_finding:\n",
    "        labels_list.append(\"Finding\")\n",
    "    labels_str = \", \".join(labels_list)\n",
    "\n",
    "    drop_bits = [\n",
    "        \"- Meta phrases (e.g., 'possible side effects', 'side effects' when not the patient’s actual event).\",\n",
    "        \"- Generic terms like 'drug', 'medicine', 'medication', 'tablet', 'pill' unless a real brand/generic product name.\",\n",
    "        \"- Dosing schedules ('twice per day', '2x/day', 'every morning').\",\n",
    "        \"- Negated mentions in the same clause ('without bleeding', 'no cramps').\",\n",
    "    ]\n",
    "    if not allow_procedures:\n",
    "        drop_bits.append(\"- Procedures/plans ('surgery', 'operation', 'procedure', 'injection', 'epidural steroid injection') unless clearly the adverse event itself.\")\n",
    "    if distance_policy == \"drop\":\n",
    "        drop_bits.append(\"- Distances/quantities by themselves ('100 meters', '1/2 km', '10 years').\")\n",
    "\n",
    "    drop_block = \"\\n\".join(drop_bits)\n",
    "\n",
    "    return f\"\"\"You are a clinical annotation assistant for CADEC forum posts.\n",
    "Return spans ONLY as strict JSON.\n",
    "\n",
    "ALLOWED labels (only these): {labels_str}.\n",
    "\n",
    "OFFSETS:\n",
    "- Character offsets are 0-based, end-exclusive, and MUST be within THIS CHUNK ONLY.\n",
    "- Provide one or more ranges per span for discontiguous mentions.\n",
    "- Every range MUST match exact text; do not hallucinate.\n",
    "\n",
    "WHAT TO LABEL:\n",
    "- Concrete, patient-experienced clinical events or conditions.\n",
    "- Prefer the most specific phrase ('lower abdominal pain' over 'pain').\n",
    "- If medication/brand or dosing context is present and the text indicates a side-effect, prefer ADR over Symptom.\n",
    "\n",
    "DO NOT LABEL (drop these):\n",
    "{drop_block}\n",
    "\n",
    "Sort spans by first range start then end. Keep output minimal and precise.\n",
    "\n",
    "Return JSON ONLY:\n",
    "{{\"spans\":[{{\"label\":\"{'|'.join(labels_list)}\",\"ranges\":[{{\"start\":int,\"end\":int}}],\"text\":\"verbatim from text\"}}]}}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e385fda",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "USER_TEMPLATE = \"\"\"CHUNK (local offsets 0..{n}):\n",
    "{chunk}\n",
    "\n",
    "Output JSON ONLY, no commentary.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e542e21d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# --- groq call (notebook-safe)\n",
    "def call_groq(model: str, system: str, user: str, temperature: float = 0.0) -> str:\n",
    "    import requests\n",
    "    key = os.environ[\"GROQ_API_KEY\"]  # already sanitized\n",
    "    url = f\"{GROQ_BASEURL}/chat/completions\"\n",
    "    headers = {\"Authorization\": f\"Bearer {key}\", \"Content-Type\": \"application/json\"}\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"temperature\": temperature,\n",
    "        \"response_format\": {\"type\": \"json_object\"},\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\", \"content\": user},\n",
    "        ],\n",
    "    }\n",
    "    r = requests.post(url, headers=headers, json=payload, timeout=TIMEOUT)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    return data[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4716c33",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# --- chunking\n",
    "def chunk_text(text: str, max_chars: int = MAX_CHARS) -> List[Tuple[int, str]]:\n",
    "    chunks: List[Tuple[int, str]] = []\n",
    "    i, n = 0, len(text)\n",
    "    while i < n:\n",
    "        j = min(i + max_chars, n)\n",
    "        cut = text.rfind(\"\\n\", i, j)\n",
    "        if cut == -1:\n",
    "            cut = text.rfind(\". \", i, j)\n",
    "        if cut == -1 or cut <= i + int(0.5 * max_chars):\n",
    "            cut = j\n",
    "        chunks.append((i, text[i:cut]))\n",
    "        i = cut\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9a37631",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# --- json & sanitization\n",
    "WHITESPACE_RE = re.compile(r\"\\s+\")\n",
    "DURATION_RE = re.compile(r\"^\\s*\\d+\\s+(years?|months?|weeks?|days?)\\b\", re.I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea45833a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def parse_json_strict(s: str) -> Dict[str, Any]:\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except Exception:\n",
    "        m = re.search(r\"\\{.*\\}$\", s.strip(), flags=re.S) or re.search(r\"\\{.*\\}\", s, flags=re.S)\n",
    "        if not m:\n",
    "            raise\n",
    "        return json.loads(m.group(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce6370bb",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def sanitize_raw_spans(resp: Dict[str, Any], chunk_len: int, allowed_labels: List[str]) -> List[RawSpan]:\n",
    "    out: List[RawSpan] = []\n",
    "    for item in (resp.get(\"spans\") or []):\n",
    "        label = item.get(\"label\")\n",
    "        if label not in allowed_labels:\n",
    "            continue\n",
    "        text = (item.get(\"text\") or \"\").strip()\n",
    "        ranges: List[Range] = []\n",
    "        for r in (item.get(\"ranges\") or []):\n",
    "            try:\n",
    "                s = int(r[\"start\"]); e = int(r[\"end\"])\n",
    "            except Exception:\n",
    "                continue\n",
    "            if not (0 <= s < e <= chunk_len):\n",
    "                continue\n",
    "            ranges.append(Range(s, e))\n",
    "        if not ranges:\n",
    "            continue\n",
    "        uniq, seen = [], set()\n",
    "        for rr in sorted(ranges, key=lambda x: (x.start, x.end)):\n",
    "            key = (rr.start, rr.end)\n",
    "            if key in seen: continue\n",
    "            seen.add(key); uniq.append(rr)\n",
    "        out.append(RawSpan(label=label, ranges=uniq, text=text))\n",
    "    out.sort(key=lambda sp: (sp.ranges[0].start, sp.ranges[0].end))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d13dba0b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# --- offset repair\n",
    "def norm(s: str) -> str:\n",
    "    return WHITESPACE_RE.sub(\" \", s).strip().casefold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22c1715f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def slice_join(chunk: str, ranges: List[Range]) -> str:\n",
    "    return \" \".join(chunk[r.start:r.end] for r in ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8b29947",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def find_exact(chunk: str, needle: str) -> Optional[Tuple[int, int]]:\n",
    "    if not needle:\n",
    "        return None\n",
    "    i = chunk.find(needle)\n",
    "    if i != -1:\n",
    "        return (i, i + len(needle))\n",
    "    lc = chunk.casefold()\n",
    "    ln = needle.casefold()\n",
    "    j = lc.find(ln)\n",
    "    if j != -1:\n",
    "        return (j, j + len(needle))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6fb6eae3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def expand_to_word_boundaries(chunk: str, s: int, e: int) -> Tuple[int, int]:\n",
    "    while s > 0 and (chunk[s-1].isalnum() or chunk[s-1] in \"'-\"):\n",
    "        s -= 1\n",
    "    n = len(chunk)\n",
    "    while e < n and (chunk[e].isalnum() or chunk[e] in \"'-\"):\n",
    "        e += 1\n",
    "    return s, e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "419e7c97",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def is_plausible_text(txt: str) -> bool:\n",
    "    t = txt.strip()\n",
    "    if len(t) < 2: return False\n",
    "    letters = sum(ch.isalpha() for ch in t)\n",
    "    if letters == 0: return False\n",
    "    return letters / max(1, len(t)) >= 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0cb2a95a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def repair_raw_span(raw: RawSpan, chunk: str) -> Optional[Span]:\n",
    "    joined = slice_join(chunk, raw.ranges)\n",
    "    if norm(joined) == norm(raw.text):\n",
    "        return Span(label=raw.label, ranges=raw.ranges)\n",
    "    match = find_exact(chunk, raw.text)\n",
    "    if match:\n",
    "        s, e = match\n",
    "        s, e = expand_to_word_boundaries(chunk, s, e)\n",
    "        surf = chunk[s:e]\n",
    "        if is_plausible_text(surf):\n",
    "            return Span(label=raw.label, ranges=[Range(s, e)])\n",
    "    s = min(r.start for r in raw.ranges)\n",
    "    e = max(r.end for r in raw.ranges)\n",
    "    s, e = expand_to_word_boundaries(chunk, s, e)\n",
    "    surf = chunk[s:e]\n",
    "    if is_plausible_text(surf):\n",
    "        return Span(label=raw.label, ranges=[Range(s, e)])\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b8ada06",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# --- global utils\n",
    "def to_global(spans: List[Span], base: int) -> List[Span]:\n",
    "    return [Span(sp.label, [Range(base + r.start, base + r.end) for r in sp.ranges]) for sp in spans]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "133f83c3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def span_hull(sp: Span) -> Tuple[int, int]:\n",
    "    s = min(r.start for r in sp.ranges)\n",
    "    e = max(r.end for r in sp.ranges)\n",
    "    return s, e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f5cedf90",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def resolve_conflicts(spans: List[Span], keep_finding: bool) -> List[Span]:\n",
    "    priority = PRIORITY_WITH_FINDING if keep_finding else PRIORITY_BASE\n",
    "    spans = sorted(spans, key=lambda sp: (span_hull(sp)[0], -(span_hull(sp)[1]-span_hull(sp)[0]), -priority.get(sp.label, 0)))\n",
    "    kept: List[Span] = []\n",
    "    for sp in spans:\n",
    "        s, e = span_hull(sp)\n",
    "        conflict = False\n",
    "        for kp in kept:\n",
    "            ks, ke = span_hull(kp)\n",
    "            if not (e <= ks or ke <= s):\n",
    "                if priority.get(kp.label, 0) > priority.get(sp.label, 0):\n",
    "                    conflict = True; break\n",
    "                if priority.get(kp.label, 0) == priority.get(sp.label, 0) and (ke-ks) >= (e-s):\n",
    "                    conflict = True; break\n",
    "        if not conflict:\n",
    "            kept.append(sp)\n",
    "    uniq, seen = [], set()\n",
    "    for sp in kept:\n",
    "        key = (sp.label, tuple((r.start, r.end) for r in sp.ranges))\n",
    "        if key in seen: continue\n",
    "        seen.add(key); uniq.append(sp)\n",
    "    uniq.sort(key=lambda sp: (span_hull(sp)[0], span_hull(sp)[1], -priority.get(sp.label, 0)))\n",
    "    return uniq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bea571a5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def clip_to_text(spans: List[Span], text: str) -> List[Span]:\n",
    "    n = len(text); out: List[Span] = []\n",
    "    for sp in spans:\n",
    "        rr = [r for r in sp.ranges if 0 <= r.start < r.end <= n]\n",
    "        if rr:\n",
    "            out.append(Span(sp.label, rr))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e800a714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- policy-aware post-filters\n",
    "META_PHRASES_RE = re.compile(r\"\\b(side effect|side effects|possible side effects?)\\b\", re.I)\n",
    "FREQUENCY_RE = re.compile(r\"\\b(?:once|twice|thrice|\\d+\\s*(?:x|times?))\\s*(?:per|a)\\s*(?:day|week|month|hour)s?\\b\", re.I)\n",
    "DISTANCE_SURF_RE = re.compile(r\"^\\s*\\d+(?:\\.\\d+)?\\s*(?:m|km|meter|meters|kilometer|kilometers|kms?)\\s*$\", re.I)\n",
    "PROCEDURE_RE = re.compile(r\"\\b(surgery|operation|procedure|injection|epidural|steroid injection)\\b\", re.I)\n",
    "GENERIC_DRUG_SURF_RE = re.compile(r\"^(?:\\b(this|that|the|my|his|her)\\b\\s+)?\\b(drug|medicine|medication|tablet|pill)s?\\b$\", re.I)\n",
    "WEEKDAY_RE = re.compile(r\"^(mon(day)?|tue(sday)?|wed(nesday)?|thu(rsday)?|fri(day)?|sat(urday)?|sun(day)?)$\", re.I)\n",
    "NON_EVENT_PHRASE_RE = re.compile(r\"\\b(this|that|the)\\s+poison\\b\", re.I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ac076a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYMPTOM_HEAD_RE = re.compile(\n",
    "    r\"\\b(pain|cramps?|bleeding|nausea|vomit(?:ing)?|diarr(?:hea|hoea)|diah?rea|diarh?ea|headache|dizz(?:y|iness)|rash|swelling)\\b\", re.I\n",
    ")\n",
    "MENSTRUAL_RE = re.compile(r\"\\b(menstrual|menstruation|periods?|menorrhagia|vaginal bleeding|bleeding from the vagina)\\b\", re.I)\n",
    "MENSTRUAL_EXTRA_RE = re.compile(r\"\\b(menstrual cramps?|vaginal cramps?|uter(?:us|ine) contractions?)\\b\", re.I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "22d63305",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOSAGE_RE = re.compile(r\"\\b\\d+\\s*(?:mg|mcg|g|ml|iu|units?|caps?|tabs?)\\b\", re.I)\n",
    "MED_CUES_RE = re.compile(r\"\\b(took|taking|dose|dosing|tablet|pill|medication|medicine|nsaid|ibuprofen|advil|naproxen|arthrotec|drug)\\b\", re.I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ae659382",
   "metadata": {},
   "outputs": [],
   "source": [
    "FUNC_VERBS = {\"walk\", \"walking\", \"run\", \"running\", \"stand\", \"standing\", \"lift\", \"able\", \"unable\", \"can\", \"can't\", \"cannot\", \"limited\", \"limit\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "565c3535",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "DRUG_LEXICON = {\n",
    "    \"Arthrotec\",\"Misoprostol\",\"Diclofenac\",\"Ibuprofen\",\"Paracetamol\",\n",
    "    \"Advil\",\"Naproxen\",\"Voltaren\",\"Lyrica\",\"Lipitor\",\"Cymbalta\",\n",
    "    \"Clonidine\",\"Tylenol\",\"co-codamol\",\"Pamprin\"\n",
    "}\n",
    "DRUG_LEXICON_RE = re.compile(r\"\\b(\" + \"|\".join(sorted(map(re.escape, DRUG_LEXICON))) + r\")\\b\", re.I)\n",
    "PROPER_BRAND_RE = re.compile(r\"^[A-Z][A-Za-z0-9\\-]{2,}$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c05f4c12",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def build_span_text(text: str, ranges: List[Range]) -> str:\n",
    "    return \" \".join(text[r.start:r.end].replace(\"\\n\", \" \").replace(\"\\t\", \" \") for r in ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6063eb91",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def has_med_context(text: str) -> bool:\n",
    "    return bool(DOSAGE_RE.search(text) or MED_CUES_RE.search(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "85e0eed8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def has_local_med_context(text: str, s: int, e: int, window: int = 120) -> bool:\n",
    "    L = max(0, s - window); R = min(len(text), e + window)\n",
    "    ctx = text[L:R]\n",
    "    return bool(MED_CUES_RE.search(ctx) or DOSAGE_RE.search(ctx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4eed1bc2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def has_functional_context(text: str, s: int, e: int, window: int = 60) -> bool:\n",
    "    n = len(text); L = max(0, s - window); R = min(n, e + window)\n",
    "    ctx = text[L:R].casefold()\n",
    "    return any(v in ctx for v in FUNC_VERBS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cab3fe94",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def post_filter_spans(\n",
    "    spans: List[Span],\n",
    "    text: str,\n",
    "    keep_finding: bool,\n",
    "    allow_procedures: bool,\n",
    "    distance_policy: str,\n",
    "    menstrual_policy: str,\n",
    "    disease_symptom_relabel: str,\n",
    ") -> List[Span]:\n",
    "    out: List[Span] = []\n",
    "    med_ctx = has_med_context(text)\n",
    "    allowed_labels = set(BASE_LABELS) if not keep_finding else {\"ADR\", \"Drug\", \"Disease\", \"Symptom\", \"Finding\"}\n",
    "\n",
    "    for sp in spans:\n",
    "        if sp.label not in allowed_labels:\n",
    "            continue\n",
    "\n",
    "        surf = build_span_text(text, sp.ranges).strip()\n",
    "        if len(surf) < 2:\n",
    "            continue\n",
    "\n",
    "        s0 = min(r.start for r in sp.ranges)\n",
    "        e0 = max(r.end for r in sp.ranges)\n",
    "\n",
    "        if sp.label == \"Drug\" and WEEKDAY_RE.fullmatch(surf):\n",
    "            continue\n",
    "        if sp.label == \"ADR\" and NON_EVENT_PHRASE_RE.search(surf):\n",
    "            continue\n",
    "\n",
    "        if META_PHRASES_RE.search(surf): \n",
    "            continue\n",
    "        if FREQUENCY_RE.search(surf): \n",
    "            continue\n",
    "\n",
    "        if not allow_procedures and PROCEDURE_RE.search(surf):\n",
    "            continue\n",
    "\n",
    "        if distance_policy == \"drop\" and DISTANCE_SURF_RE.match(surf):\n",
    "            continue\n",
    "        elif distance_policy == \"functional\" and DISTANCE_SURF_RE.match(surf):\n",
    "            if not has_functional_context(text, s0, e0):\n",
    "                continue\n",
    "            if keep_finding and sp.label != \"Finding\":\n",
    "                sp = Span(label=\"Finding\", ranges=sp.ranges)\n",
    "\n",
    "        if sp.label == \"Drug\":\n",
    "            longish = (\",\" in surf) or (len(surf.split()) > 4)\n",
    "            looks_brand = bool(DRUG_LEXICON_RE.search(surf) or PROPER_BRAND_RE.fullmatch(surf.strip()))\n",
    "            dosage_near = bool(DOSAGE_RE.search(text[max(0, s0-40):min(len(text), e0+40)]))\n",
    "            if GENERIC_DRUG_SURF_RE.match(surf):\n",
    "                continue\n",
    "            if longish and not (looks_brand or dosage_near):\n",
    "                continue\n",
    "\n",
    "        if DURATION_RE.match(surf):\n",
    "            continue\n",
    "\n",
    "        if disease_symptom_relabel == \"on\" and sp.label == \"Disease\" and SYMPTOM_HEAD_RE.search(surf):\n",
    "            sp = Span(label=(\"ADR\" if med_ctx else \"Symptom\"), ranges=sp.ranges)\n",
    "\n",
    "        if MENSTRUAL_RE.search(surf) or MENSTRUAL_EXTRA_RE.search(surf):\n",
    "            if menstrual_policy == \"adr\":\n",
    "                sp = Span(label=\"ADR\", ranges=sp.ranges)\n",
    "            elif menstrual_policy == \"symptom\":\n",
    "                sp = Span(label=\"Symptom\", ranges=sp.ranges)\n",
    "            else:\n",
    "                sp = Span(label=(\"ADR\" if med_ctx else \"Symptom\"), ranges=sp.ranges)\n",
    "\n",
    "        if sp.label == \"Symptom\" and SYMPTOM_HEAD_RE.search(surf):\n",
    "            if med_ctx or has_local_med_context(text, s0, e0, window=120):\n",
    "                sp = Span(label=\"ADR\", ranges=sp.ranges)\n",
    "\n",
    "        if sp.label == \"ADR\":\n",
    "            if DRUG_LEXICON_RE.search(surf) or PROPER_BRAND_RE.fullmatch(surf):\n",
    "                sp = Span(label=\"Drug\", ranges=sp.ranges)\n",
    "\n",
    "        out.append(sp)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "47ed25b1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# --- enumeration splitter\n",
    "LIST_SEP_RE = re.compile(r\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f6580ca4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def _trim_to_text_bounds(text: str, s: int, e: int) -> tuple[int, int]:\n",
    "    while s < e and text[s].isspace(): s += 1\n",
    "    while e > s and text[e-1].isspace(): e -= 1\n",
    "    while s > 0 and (text[s-1].isalnum() or text[s-1] in \"'-\"): s -= 1\n",
    "    n = len(text)\n",
    "    while e < n and (text[e].isalnum() or text[e] in \"'-\"): e += 1\n",
    "    return s, e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5c3627a6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def _looks_like_item(surf: str) -> bool:\n",
    "    surf = surf.strip()\n",
    "    if len(surf) < 2: return False\n",
    "    alpha = sum(ch.isalpha() for ch in surf)\n",
    "    return alpha >= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bef101c2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def split_enumerations(spans: List[Span], text: str,\n",
    "                       labels_to_split: Iterable[str] = (\"Symptom\", \"ADR\"),\n",
    "                       also_split_and: bool = False) -> List[Span]:\n",
    "    out: List[Span] = []\n",
    "    for sp in spans:\n",
    "        if sp.label not in labels_to_split or len(sp.ranges) != 1:\n",
    "            out.append(sp); continue\n",
    "        r = sp.ranges[0]\n",
    "        chunk = text[r.start:r.end]\n",
    "        if (\",\" not in chunk) and not (also_split_and and \" and \" in chunk.lower()):\n",
    "            out.append(sp); continue\n",
    "\n",
    "        parts: List[tuple[int,int]] = []\n",
    "        last = 0\n",
    "        for m in LIST_SEP_RE.finditer(chunk):\n",
    "            parts.append((last, m.start())); last = m.end()\n",
    "        parts.append((last, len(chunk)))\n",
    "\n",
    "        items: List[tuple[int,int]] = []\n",
    "        for (ps, pe) in parts:\n",
    "            sub = chunk[ps:pe]\n",
    "            if also_split_and and \" and \" in sub.lower() and \",\" not in sub:\n",
    "                idx = sub.lower().find(\" and \")\n",
    "                if idx != -1:\n",
    "                    items.append((ps, ps+idx)); items.append((ps+idx+5, pe))\n",
    "                else:\n",
    "                    items.append((ps, pe))\n",
    "            else:\n",
    "                items.append((ps, pe))\n",
    "\n",
    "        children: List[Span] = []\n",
    "        for (ps, pe) in items:\n",
    "            s = r.start + ps; e = r.start + pe\n",
    "            s, e = _trim_to_text_bounds(text, s, e)\n",
    "            if e <= s: continue\n",
    "            surf = text[s:e]\n",
    "            if not _looks_like_item(surf): continue\n",
    "            children.append(Span(sp.label, [Range(s, e)]))\n",
    "\n",
    "        if len(children) >= 2: out.extend(children)\n",
    "        else: out.append(sp)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "722d2e0d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# --- writer (.ann)\n",
    "def to_brat_ann_lines(spans: Iterable[Span], text: str) -> List[str]:\n",
    "    lines: List[str] = []\n",
    "    i = 1\n",
    "    for sp in spans:\n",
    "        coords = \";\".join(f\"{r.start} {r.end}\" for r in sp.ranges)\n",
    "        surf = build_span_text(text, sp.ranges)\n",
    "        lines.append(f\"T{i}\\t{sp.label} {coords}\\t{surf}\")\n",
    "        i += 1\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a1833364",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# --- orchestration\n",
    "def process_one_file(\n",
    "    path: Path,\n",
    "    model: str,\n",
    "    out_dir: Path,\n",
    "    temperature: float,\n",
    "    max_chars: int,\n",
    "    verbose: bool,\n",
    "    keep_finding: bool,\n",
    "    allow_procedures: bool,\n",
    "    distance_policy: str,\n",
    "    menstrual_policy: str,\n",
    "    disease_symptom_relabel: str,\n",
    "    split_enums: bool,\n",
    "    split_and: bool,\n",
    ") -> Path:\n",
    "    raw = path.read_text(encoding=\"utf-8\")\n",
    "    chunks = chunk_text(raw, max_chars=max_chars)\n",
    "\n",
    "    labels_list = list(BASE_LABELS)\n",
    "    if keep_finding:\n",
    "        labels_list.append(\"Finding\")\n",
    "\n",
    "    system_prompt = build_system_prompt(keep_finding, allow_procedures, distance_policy)\n",
    "\n",
    "    all_spans: List[Span] = []\n",
    "    for base, chunk in chunks:\n",
    "        user = USER_TEMPLATE.format(n=len(chunk), chunk=chunk)\n",
    "        resp_text = call_groq(model, system_prompt, user, temperature=temperature)\n",
    "        try:\n",
    "            data = parse_json_strict(resp_text)\n",
    "        except Exception as ex:\n",
    "            if verbose:\n",
    "                print(f\"[WARN] JSON parse failed @ {path.name} chunk@{base}: {ex}\\n{resp_text[:400]}\")\n",
    "            continue\n",
    "        raw_spans = sanitize_raw_spans(data, len(chunk), labels_list)\n",
    "\n",
    "        repaired: List[Span] = []\n",
    "        for rs in raw_spans:\n",
    "            sp = repair_raw_span(rs, chunk)\n",
    "            if sp is not None:\n",
    "                repaired.append(sp)\n",
    "\n",
    "        global_spans = to_global(repaired, base)\n",
    "        all_spans.extend(global_spans)\n",
    "\n",
    "    all_spans = clip_to_text(all_spans, raw)\n",
    "    all_spans = post_filter_spans(\n",
    "        all_spans,\n",
    "        raw,\n",
    "        keep_finding=keep_finding,\n",
    "        allow_procedures=allow_procedures,\n",
    "        distance_policy=distance_policy,\n",
    "        menstrual_policy=menstrual_policy,\n",
    "        disease_symptom_relabel=disease_symptom_relabel,\n",
    "    )\n",
    "\n",
    "    if split_enums:\n",
    "        all_spans = split_enumerations(all_spans, raw, labels_to_split=(\"Symptom\", \"ADR\"), also_split_and=split_and)\n",
    "\n",
    "    final_spans = resolve_conflicts(all_spans, keep_finding=keep_finding)\n",
    "    ann_lines = to_brat_ann_lines(final_spans, raw)\n",
    "\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out_path = out_dir / (path.stem + \".ann\")\n",
    "    out_path.write_text(\"\\n\".join(ann_lines) + (\"\\n\" if ann_lines else \"\"), encoding=\"utf-8\")\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\n--- {path.name} ({len(raw)} chars) ---\")\n",
    "        for ln in ann_lines:\n",
    "            print(ln)\n",
    "    print(f\"[OK] {path.name}: {len(final_spans)} spans → {out_path}\")\n",
    "    return out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d41a4a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all(\n",
    "    text_dir: Path,\n",
    "    model: str,\n",
    "    out_dir: Path,\n",
    "    **kw,\n",
    "):\n",
    "    files = sorted([p for p in text_dir.iterdir() if p.suffix.lower() == \".txt\"])\n",
    "    total, ok = 0, 0\n",
    "    for p in files:\n",
    "        total += 1\n",
    "        try:\n",
    "            out = process_one_file(p, model, out_dir, **kw)\n",
    "            if out.exists() and out.stat().st_size > 0:\n",
    "                ok += 1\n",
    "        except Exception as ex:\n",
    "            print(f\"[WARN] {p.name}: {ex}\")\n",
    "    print(f\"Done: {ok}/{total} files produced spans.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "25fdd94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ARTHROTEC.20.txt (634 chars) ---\n",
      "T1\tADR 19 34\tlower back pain\n",
      "T2\tDrug 64 78\tarthrotec 50mg\n",
      "T3\tADR 197 213\tmenstrual cramps\n",
      "T4\tADR 224 232\tbleeding\n",
      "T5\tSymptom 249 257\tsickness\n",
      "T6\tSymptom 269 275\tpuking\n",
      "[OK] ARTHROTEC.20.txt: 6 spans → /Users/anjalikulkarni/Desktop/Assignment1/predicted/ARTHROTEC.20.ann\n",
      "\n",
      "--- .ann preview ---\n",
      "T1\tADR 19 34\tlower back pain\n",
      "T2\tDrug 64 78\tarthrotec 50mg\n",
      "T3\tADR 197 213\tmenstrual cramps\n",
      "T4\tADR 224 232\tbleeding\n",
      "T5\tSymptom 249 257\tsickness\n",
      "T6\tSymptom 269 275\tpuking\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# === your paths ===\n",
    "TEXT_DIR = Path(\"/Users/anjalikulkarni/Desktop/Assignment1/CADEC-lPWNPfjE-/data/cadec/text\")\n",
    "FILE_NAME = \"ARTHROTEC.20.txt\"  # set to \"all\" to process all files\n",
    "OUT_DIR  = Path(\"/Users/anjalikulkarni/Desktop/Assignment1/predicted\")\n",
    "MODEL    = DEFAULT_MODEL\n",
    "\n",
    "# === policy flags (match your CLI defaults) ===\n",
    "kwargs = dict(\n",
    "    temperature=0.0,\n",
    "    max_chars=MAX_CHARS,\n",
    "    verbose=True,\n",
    "    keep_finding=False,\n",
    "    allow_procedures=False,\n",
    "    distance_policy=\"drop\",\n",
    "    menstrual_policy=\"auto\",\n",
    "    disease_symptom_relabel=\"on\",\n",
    "    split_enums=True,\n",
    "    split_and=False,\n",
    ")\n",
    "\n",
    "if FILE_NAME.lower() == \"all\":\n",
    "    process_all(TEXT_DIR, MODEL, OUT_DIR, **kwargs)\n",
    "else:\n",
    "    path = TEXT_DIR / FILE_NAME\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(path)\n",
    "    out_path = process_one_file(path, MODEL, OUT_DIR, **kwargs)\n",
    "    print(\"\\n--- .ann preview ---\")\n",
    "    print((OUT_DIR / (Path(FILE_NAME).stem + \".ann\")).read_text(encoding=\"utf-8\"))\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
